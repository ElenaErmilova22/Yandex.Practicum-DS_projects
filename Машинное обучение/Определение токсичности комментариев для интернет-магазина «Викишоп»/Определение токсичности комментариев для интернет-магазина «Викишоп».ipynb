{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Аннотация:-проект-для-«Викишоп»\" data-toc-modified-id=\"Аннотация:-проект-для-«Викишоп»-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Аннотация: проект для «Викишоп»</a></span></li><li><span><a href=\"#Знакомство-с-данными\" data-toc-modified-id=\"Знакомство-с-данными-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Знакомство с данными</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы:\" data-toc-modified-id=\"Выводы:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Выводы:</a></span></li></ul></li><li><span><a href=\"#Предобработка-данных\" data-toc-modified-id=\"Предобработка-данных-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Предобработка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы:\" data-toc-modified-id=\"Выводы:-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Выводы:</a></span></li></ul></li><li><span><a href=\"#Обучение-моделей\" data-toc-modified-id=\"Обучение-моделей-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Обучение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы:\" data-toc-modified-id=\"Выводы:-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Выводы:</a></span></li></ul></li><li><span><a href=\"#Финальное-тестирование\" data-toc-modified-id=\"Финальное-тестирование-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Финальное тестирование</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы:\" data-toc-modified-id=\"Выводы:-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Выводы:</a></span></li></ul></li><li><span><a href=\"#Итоговые-выводы\" data-toc-modified-id=\"Итоговые-выводы-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Итоговые выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аннотация: проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "**Цель** проекта: на основе набора данных с разметкой о токсичности правок построить и обучить модель для классификации комментариев на позитивные и негативные. \n",
    "\n",
    "**Задачи** проекта: \n",
    "- обучить несколько моделей машинного обучения;\n",
    "- обеспечить достижения метрикой качества *F1* значения не меньше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем библиотеки и инструменты, которые понадобятся нам при выполнении проекта\n",
    "import pandas as pd\n",
    "import re \n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "(159571, 2)\n",
      "-------------------------\n",
      "text     0\n",
      "toxic    0\n",
      "dtype: int64\n",
      "-------------------------\n",
      "Количество явных дубликатов: 0\n",
      "-------------------------\n",
      "0    143346\n",
      "1     16225\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#применяем специальный метод для чтения исходного файла\n",
    "initial_data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "#выводим на экран ключевые данные об исходном датасете\n",
    "display(initial_data.head(10))\n",
    "print('-'*25)\n",
    "print(initial_data.shape)\n",
    "print('-'*25)\n",
    "print(initial_data.isnull().sum())\n",
    "print('-'*25)\n",
    "print('Количество явных дубликатов:', initial_data.duplicated().sum())\n",
    "print('-'*25)\n",
    "print(initial_data['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знакомство с исходными данными позволило выявить следующее:\n",
    "- комментарии написаны на английском языке;\n",
    "- тексты комментариев не очищены, часто встречается символ новой строки;\n",
    "- в текстах использованы буквы различных регистров;\n",
    "- пропусков и явных дубликатов не обнаружено;\n",
    "- наблюдается дисбаланс классов целевого признака.\n",
    "\n",
    "На следующем этапе обработаем тексты комментариев для улучшения их качества в целях машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках предобработки исходных данных очистим тексты комментариев от знаков новой строки, приведем все символы к нижнему регистру, оставим в них только символы латиницы и пробелы, произведем лемматизацию.\n",
    "\n",
    "Начнем с очистки текстов, создадим специальную функцию, которая выполнит выше названные задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore') #отключим предупреждения на случай возникновения в моделях параметров, при которых \n",
    "                                  #не удастся вычислить F1-меру\n",
    "        \n",
    "def clear_text(text): #создадим функцию для очистки текстов комментариев\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    return text\n",
    "\n",
    "initial_data['new_text'] = initial_data['text'].apply(clear_text) #применим функцию к столбцу с текстами, создав новый \n",
    "                                                                  #столбец, чтобы была возможность вернуться к исходным\n",
    "                                                                  #текстам и провести сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее лемматизируем тексты. Так как комментарии оставлены на английском языке, то используемые нами ранее инструменты для лемматизации не подойдут. Используем WordNetLemmatizer из библиотеки NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from me a well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense be offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which be contrar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                            new_text  \n",
       "0  explanation why the edits make under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not try to edit war it s ju...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir be my hero any chance you remember wha...  \n",
       "5  congratulation from me a well use the tool wel...  \n",
       "6       cocksucker before you piss around on my work  \n",
       "7  your vandalism to the matt shirvington article...  \n",
       "8  sorry if the word nonsense be offensive to you...  \n",
       "9  alignment on this subject and which be contrar...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество явных дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(word): #создадим функцию для определения части речи слов из текстов\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_word_text (corpus): #создадим функцию для токенизации и лемматизации корпуса текстов\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(corpus)])\n",
    "    return lemmatized\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "initial_data['new_text'] = initial_data['new_text'].apply(get_word_text) #применим функцию к очищенным комментариям\n",
    "\n",
    "display(initial_data.head(10)) #выведем результат преобразований на экран\n",
    "print()\n",
    "print('Количество явных дубликатов:', initial_data.duplicated().sum()) #еще раз проверим на явные дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разделим имеющиеся у нас данные на обучающую, валидационную и тестовую выборки в соотношении 3:1:1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Очистка и лемматизация были сделаны верно.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95742,) (31915,) (31914,)\n"
     ]
    }
   ],
   "source": [
    "#выделяем признаки и целевой признак\n",
    "features = initial_data['new_text']\n",
    "target = initial_data['toxic']\n",
    "\n",
    "#делим данные на выборки\n",
    "features, features_test, target, target_test = train_test_split(features, target, test_size = 0.2, random_state = 555)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size = 0.25, random_state = 555)\n",
    "\n",
    "print(features_train.shape, features_test.shape,  features_valid.shape) #проверяем оотношение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этапе предобработки данных была произведена очистка и лемматизация текстов комментариев, а также приведение всех символов в текстах к нижнему регистру. В результате исходный фрейм дополнился столбцом с текстами, готовыми для использования в качестве признаков при обучении, проверке и тестировании моделей машинного обучения. \n",
    "\n",
    "Кроме того в отдельные переменные были выделены признаки и целевой признак, а данные разделены на три выборки: обучающую, валидационную и тестовую. \n",
    "\n",
    "Можем переходить к обучению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном проекте мы обучим две модели: логистическую регрессию и CatBoost. Т.к. нам нужно определить тональность текстов комментариев, то в качестве признаков будут использованы величины TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer() #вызываем счетчик\n",
    "\n",
    "#определяем величины TF-IDF признаков в выборках\n",
    "tf_idf_train = tf_idf.fit_transform(features_train) \n",
    "tf_idf_valid = tf_idf.transform(features_valid) \n",
    "tf_idf_test = tf_idf.transform(features_test) \n",
    "\n",
    "#обучаем модель логистической регрессии, балансируем классы\n",
    "model = LogisticRegression (class_weight ='balanced', random_state = 555)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "predictions = model.predict(tf_idf_valid)\n",
    "f1_regress = f1_score(target_valid, predictions) \n",
    "print(f1_regress.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.046956\n",
      "0:\tlearn: 0.4218737\ttotal: 3.79s\tremaining: 1h 40m 54s\n",
      "500:\tlearn: 0.7463637\ttotal: 27m 20s\tremaining: 59m 59s\n",
      "1000:\tlearn: 0.7925708\ttotal: 53m 46s\tremaining: 32m 10s\n",
      "1500:\tlearn: 0.8166903\ttotal: 1h 20m 31s\tremaining: 5m 18s\n",
      "1599:\tlearn: 0.8197069\ttotal: 1h 26m 13s\tremaining: 0us\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "#обучаем модель CatBoost, параметры подбираем вручную\n",
    "cat_model = CatBoostClassifier(eval_metric='F1', iterations = 1600)\n",
    "cat_model.fit(tf_idf_train, target_train, verbose = 500)\n",
    "pred_valid = cat_model.predict(tf_idf_valid) \n",
    "cat_result = f1_score(target_valid, pred_valid) \n",
    "print(cat_result.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При проверке на валидационной выборке модель логистической регрессии не смогла преодолеть пороговое значение метрики F1. В то же время модель CatBoost показала лучший результат, достигнув значения 0.77. \n",
    "\n",
    "На следующем этапе произведем финальное тестирование моделей и выберем из них наиболее подходящую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Финальное тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим предсказания обученных ранее моделей на тестовой выборке и подсчитаем значение метрики качества. Финальное тестирование будет проводиться в том же порядке, что и обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат логистической регрессии: 0.75\n"
     ]
    }
   ],
   "source": [
    "predictions_test = model.predict(tf_idf_test)\n",
    "f1_regress_test = f1_score(target_test, predictions_test) \n",
    "print('Результат логистической регрессии:', f1_regress_test.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат CatBoost: 0.76\n"
     ]
    }
   ],
   "source": [
    "pred_test = cat_model.predict(tf_idf_test)\n",
    "cat_result_test = f1_score(target_test, pred_test) \n",
    "print('Результат CatBoost:', cat_result_test.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель логистической регрессии на тестовой выборке показала результат лучше, чем на валидационной, однако даже при этом она смогла достичь лишь нижней границы допустимых значений метрики F1. \n",
    "\n",
    "Результат модели CatBoost наоборот снизился по сравнению с предыдущим этапом, при этом он все равно выше, чем у логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоговые выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе выполения проекта по построению модели, классифицирующей комментарии на позитивные и негативные, были решены следующие задачи:\n",
    "- произведена обработка текстов комментариев с целью улучшения их качества для дальнейшего использования в машинном обучении: очистка, токенизация, лемматизация;\n",
    "- обучены две модели машинного обучения - линейная регрессия и СatBoost;\n",
    "- произведена финальная проверка отобранных моделей на тестовой выборке;\n",
    "- оценено качество полученных предсказаний.\n",
    "\n",
    "В результате, искомой моделью стала модель CatBoost с количеством итераций, равным 1600. Значение метрики качества данной модели на тестовой выборке составило 0.76."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1739,
    "start_time": "2022-05-05T12:59:08.583Z"
   },
   {
    "duration": 184,
    "start_time": "2022-05-05T12:59:10.324Z"
   },
   {
    "duration": 787,
    "start_time": "2022-05-05T13:00:01.013Z"
   },
   {
    "duration": 1063,
    "start_time": "2022-05-05T13:16:56.805Z"
   },
   {
    "duration": 1008,
    "start_time": "2022-05-05T13:17:17.052Z"
   },
   {
    "duration": 1037,
    "start_time": "2022-05-05T13:17:58.271Z"
   },
   {
    "duration": 1582,
    "start_time": "2022-05-05T13:21:26.744Z"
   },
   {
    "duration": 14,
    "start_time": "2022-05-05T13:42:17.175Z"
   },
   {
    "duration": 548,
    "start_time": "2022-05-05T13:43:44.755Z"
   },
   {
    "duration": 216,
    "start_time": "2022-05-05T13:43:51.362Z"
   },
   {
    "duration": 1467,
    "start_time": "2022-05-05T13:43:54.090Z"
   },
   {
    "duration": 27,
    "start_time": "2022-05-05T13:47:29.864Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-05T13:48:22.135Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-05T13:48:26.112Z"
   },
   {
    "duration": 545,
    "start_time": "2022-05-05T13:53:04.669Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-05T13:56:20.139Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-05T13:56:20.140Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-05T13:56:20.141Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-05T13:56:20.142Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-05T13:56:23.495Z"
   },
   {
    "duration": 1311,
    "start_time": "2022-05-05T13:56:23.501Z"
   },
   {
    "duration": 1595,
    "start_time": "2022-05-05T13:56:24.814Z"
   },
   {
    "duration": 27455,
    "start_time": "2022-05-05T13:56:26.412Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-05T13:57:00.056Z"
   },
   {
    "duration": 1285,
    "start_time": "2022-05-05T13:57:00.064Z"
   },
   {
    "duration": 2779,
    "start_time": "2022-05-05T13:57:01.352Z"
   },
   {
    "duration": 16195,
    "start_time": "2022-05-05T13:57:04.135Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-05T13:57:23.895Z"
   },
   {
    "duration": 1005,
    "start_time": "2022-05-05T13:57:23.901Z"
   },
   {
    "duration": 2736,
    "start_time": "2022-05-05T13:57:24.909Z"
   },
   {
    "duration": 1252231,
    "start_time": "2022-05-05T13:57:27.648Z"
   },
   {
    "duration": 44,
    "start_time": "2022-05-05T14:28:47.322Z"
   },
   {
    "duration": 51,
    "start_time": "2022-05-05T14:29:52.645Z"
   },
   {
    "duration": 50,
    "start_time": "2022-05-05T14:30:00.733Z"
   },
   {
    "duration": 55,
    "start_time": "2022-05-05T14:30:04.886Z"
   },
   {
    "duration": 681,
    "start_time": "2022-05-05T14:32:06.584Z"
   },
   {
    "duration": 858,
    "start_time": "2022-05-05T14:32:16.927Z"
   },
   {
    "duration": 15,
    "start_time": "2022-05-05T14:34:28.112Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-05T14:35:20.680Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-05T14:35:25.111Z"
   },
   {
    "duration": 68,
    "start_time": "2022-05-05T14:36:31.296Z"
   },
   {
    "duration": 1214,
    "start_time": "2022-05-05T14:40:06.753Z"
   },
   {
    "duration": 2554,
    "start_time": "2022-05-06T07:04:08.399Z"
   },
   {
    "duration": 1351,
    "start_time": "2022-05-06T07:04:10.956Z"
   },
   {
    "duration": 2331,
    "start_time": "2022-05-06T07:04:12.309Z"
   },
   {
    "duration": 1302,
    "start_time": "2022-05-06T07:04:14.642Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T07:04:15.948Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T07:04:15.951Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T07:06:11.511Z"
   },
   {
    "duration": 1225,
    "start_time": "2022-05-06T07:06:11.519Z"
   },
   {
    "duration": 2299,
    "start_time": "2022-05-06T07:06:12.747Z"
   },
   {
    "duration": 633254,
    "start_time": "2022-05-06T07:06:15.048Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T07:16:48.305Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T07:16:48.307Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T07:16:50.975Z"
   },
   {
    "duration": 1428,
    "start_time": "2022-05-06T07:16:50.983Z"
   },
   {
    "duration": 2312,
    "start_time": "2022-05-06T07:16:52.413Z"
   },
   {
    "duration": 1959587,
    "start_time": "2022-05-06T07:16:54.727Z"
   },
   {
    "duration": 45,
    "start_time": "2022-05-06T07:49:34.317Z"
   },
   {
    "duration": 1557,
    "start_time": "2022-05-06T07:49:34.364Z"
   },
   {
    "duration": 29,
    "start_time": "2022-05-06T08:16:00.683Z"
   },
   {
    "duration": 145,
    "start_time": "2022-05-06T08:18:30.407Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T08:29:31.257Z"
   },
   {
    "duration": 31,
    "start_time": "2022-05-06T08:30:21.924Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-06T09:18:27.482Z"
   },
   {
    "duration": 1266,
    "start_time": "2022-05-06T09:18:27.492Z"
   },
   {
    "duration": 2277,
    "start_time": "2022-05-06T09:18:28.760Z"
   },
   {
    "duration": 1999677,
    "start_time": "2022-05-06T09:18:31.039Z"
   },
   {
    "duration": 93,
    "start_time": "2022-05-06T09:51:50.719Z"
   },
   {
    "duration": 102,
    "start_time": "2022-05-06T09:51:50.814Z"
   },
   {
    "duration": 45545,
    "start_time": "2022-05-06T09:52:36.897Z"
   },
   {
    "duration": 61499,
    "start_time": "2022-05-06T09:55:43.098Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T09:58:59.340Z"
   },
   {
    "duration": 2349320,
    "start_time": "2022-05-06T09:59:04.275Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-06T10:38:22.676Z"
   },
   {
    "duration": 1327,
    "start_time": "2022-05-06T10:38:22.685Z"
   },
   {
    "duration": 2376,
    "start_time": "2022-05-06T10:38:24.014Z"
   },
   {
    "duration": 1978441,
    "start_time": "2022-05-06T10:38:26.392Z"
   },
   {
    "duration": 103,
    "start_time": "2022-05-06T11:11:24.837Z"
   },
   {
    "duration": 60454,
    "start_time": "2022-05-06T11:11:24.943Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-06T16:09:18.393Z"
   },
   {
    "duration": 1286,
    "start_time": "2022-05-06T16:09:18.401Z"
   },
   {
    "duration": 201,
    "start_time": "2022-05-06T16:09:19.689Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T16:09:19.892Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T16:09:19.894Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T16:09:19.895Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T16:09:19.897Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-06T17:11:43.998Z"
   },
   {
    "duration": 1389,
    "start_time": "2022-05-06T17:11:44.005Z"
   },
   {
    "duration": 124,
    "start_time": "2022-05-06T17:11:45.397Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:11:45.524Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:11:45.525Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:11:45.527Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:11:45.529Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T17:30:45.403Z"
   },
   {
    "duration": 13,
    "start_time": "2022-05-06T17:31:24.801Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-06T17:34:44.543Z"
   },
   {
    "duration": 1328,
    "start_time": "2022-05-06T17:34:44.553Z"
   },
   {
    "duration": 2354,
    "start_time": "2022-05-06T17:34:45.884Z"
   },
   {
    "duration": 424,
    "start_time": "2022-05-06T17:34:48.240Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:34:48.667Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:34:48.668Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:34:48.670Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T17:36:10.550Z"
   },
   {
    "duration": 1472,
    "start_time": "2022-05-06T17:36:10.558Z"
   },
   {
    "duration": 2388,
    "start_time": "2022-05-06T17:36:12.033Z"
   },
   {
    "duration": 2819,
    "start_time": "2022-05-06T17:36:14.423Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:36:17.245Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:36:17.246Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T17:36:17.248Z"
   },
   {
    "duration": 1500,
    "start_time": "2022-05-06T17:36:58.449Z"
   },
   {
    "duration": 3683,
    "start_time": "2022-05-06T17:37:58.991Z"
   },
   {
    "duration": 1212,
    "start_time": "2022-05-06T17:38:02.677Z"
   },
   {
    "duration": 13812,
    "start_time": "2022-05-06T17:40:29.716Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-06T17:40:46.633Z"
   },
   {
    "duration": 1857,
    "start_time": "2022-05-06T17:40:46.643Z"
   },
   {
    "duration": 4589,
    "start_time": "2022-05-06T17:40:48.504Z"
   },
   {
    "duration": 2147975,
    "start_time": "2022-05-06T17:40:53.097Z"
   },
   {
    "duration": 1724,
    "start_time": "2022-05-06T18:16:41.082Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T18:16:42.809Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T18:16:42.810Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-06T19:24:15.871Z"
   },
   {
    "duration": 1376,
    "start_time": "2022-05-06T19:24:15.879Z"
   },
   {
    "duration": 3241,
    "start_time": "2022-05-06T19:24:17.257Z"
   },
   {
    "duration": 2222298,
    "start_time": "2022-05-06T19:24:20.501Z"
   },
   {
    "duration": 104,
    "start_time": "2022-05-06T20:01:22.803Z"
   },
   {
    "duration": 33,
    "start_time": "2022-05-06T20:01:22.909Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-06T20:01:22.945Z"
   },
   {
    "duration": 69134,
    "start_time": "2022-05-06T20:07:54.474Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-07T08:33:41.877Z"
   },
   {
    "duration": 1431,
    "start_time": "2022-05-07T08:33:41.885Z"
   },
   {
    "duration": 2285,
    "start_time": "2022-05-07T08:33:43.319Z"
   },
   {
    "duration": 2005395,
    "start_time": "2022-05-07T08:33:45.607Z"
   },
   {
    "duration": 79,
    "start_time": "2022-05-07T09:07:11.004Z"
   },
   {
    "duration": 59603,
    "start_time": "2022-05-07T09:07:11.085Z"
   },
   {
    "duration": 13260604,
    "start_time": "2022-05-07T09:08:10.691Z"
   },
   {
    "duration": 28109,
    "start_time": "2022-05-07T12:54:11.951Z"
   },
   {
    "duration": 66284,
    "start_time": "2022-05-07T12:54:50.708Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-07T12:55:56.995Z"
   },
   {
    "duration": 618,
    "start_time": "2022-05-07T12:55:57.007Z"
   },
   {
    "duration": 76260,
    "start_time": "2022-05-07T12:56:41.334Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-07T12:57:57.597Z"
   },
   {
    "duration": 630,
    "start_time": "2022-05-07T12:57:57.622Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-07T13:00:16.166Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-07T14:04:20.437Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-07T14:04:20.611Z"
   },
   {
    "duration": 1278,
    "start_time": "2022-05-07T14:04:20.619Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-07T14:58:00.199Z"
   },
   {
    "duration": 1920,
    "start_time": "2022-05-07T14:58:00.211Z"
   },
   {
    "duration": 4355,
    "start_time": "2022-05-07T14:58:02.134Z"
   },
   {
    "duration": 21997,
    "start_time": "2022-05-07T14:58:06.492Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-07T15:02:15.882Z"
   },
   {
    "duration": 1263,
    "start_time": "2022-05-07T15:02:15.895Z"
   },
   {
    "duration": 2523,
    "start_time": "2022-05-07T15:02:17.162Z"
   },
   {
    "duration": 2289382,
    "start_time": "2022-05-07T15:02:19.688Z"
   },
   {
    "duration": 96,
    "start_time": "2022-05-07T15:40:29.072Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-07T16:53:03.163Z"
   },
   {
    "duration": 1620,
    "start_time": "2022-05-07T16:53:03.173Z"
   },
   {
    "duration": 2958,
    "start_time": "2022-05-07T16:53:04.796Z"
   },
   {
    "duration": 2482988,
    "start_time": "2022-05-07T16:53:07.757Z"
   },
   {
    "duration": 169,
    "start_time": "2022-05-07T17:34:30.749Z"
   },
   {
    "duration": 69791,
    "start_time": "2022-05-07T17:34:30.921Z"
   },
   {
    "duration": 5227125,
    "start_time": "2022-05-07T17:35:40.715Z"
   },
   {
    "duration": 39,
    "start_time": "2022-05-07T19:02:47.849Z"
   },
   {
    "duration": 850,
    "start_time": "2022-05-07T19:02:47.894Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
